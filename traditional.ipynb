{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a dataset of sensor data about hand gestures and, as a way of beginning our ML journey, we will learn a single model (in traditional fashion) from all the data to predict which hand gesture is depicted. Then we will implement Google's federated learning, as above, on this data. Then, there are a number of possible extensions to the work. One is to implement some new ideas that I have had for averaging the models. Another is to move to more challenging datasets from the domain of human activity recognition, which will need use of more complex neural networks for the learning.\n",
    "\n",
    "do it for users\n",
    "\n",
    "Reference need to be colleceted\n",
    "Data needs to be \n",
    "maybe bad to have too much accuracy. hard to see if the federated approach makes a diff\n",
    "\n",
    "===========================================================================\n",
    "Questions for derek\n",
    "Do convnets learn patterns from top left and apply it to the bottom right/\n",
    "\tThe max pooling concept doesnâ€™t follow that\n",
    "Diff between rnn and residual connection\n",
    "\n",
    "===========================================================================\n",
    "Points to cover after basics\n",
    "\n",
    "Secure aggregation\n",
    "    the server doesnt know the key with which the data was encrypted\n",
    "\n",
    "===========================================================================\n",
    "\n",
    "Models to make\n",
    "    traditional model learning from all data\n",
    "    googles federated learning\n",
    "        each user builds a model locally and shares the parameters that get learned. Centrally, the parameters are averaged and the averages are sent back to the users, who can incorporate them into the next round of local learning.\n",
    "\n",
    "\n",
    "===========================================================================\n",
    "dataset\n",
    "    https://archive.ics.uci.edu/ml/datasets/MoCap+Hand+Postures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record is a set. The i-th marker of a given record does not necessarily correspond to the i-th marker of a different record. One may randomly permute the visible (not missing) markers of a given record without changing the set that the record represents. For the sake of convenience, all visible markers of a given record are given a lower index than any missing marker. A class is not guaranteed to have even a single record with all markers visible.\n",
    "\n",
    "**Information on the Class enumeration**\n",
    "\n",
    "Ranges from 1 to 5 with:\n",
    "* 1 = Fist (with thumb out), \n",
    "* 2 = Stop (hand flat), \n",
    "* 3 = Point1 (point with pointer finger), \n",
    "* 4 = Point2 (point with pointer and middle fingers), \n",
    "* 5 = Grab (fingers curled as if to grab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graphs(history_dict):\n",
    "    # this is from the book 74,75\n",
    "    # history = model.fit(...)\n",
    "    # history_dict = history.history\n",
    "\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "    plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf()\n",
    "    acc_values = history_dict['acc']\n",
    "    val_acc_values = history_dict['val_acc']\n",
    "    plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    \"return 2d df after imputing with 0s\"\n",
    "    \n",
    "    # read data\n",
    "    df = pd.read_csv(\"dataset/allUsers.lcl.csv\")\n",
    "\n",
    "    # replace teh question marks with NaN and then change data type to float 32\n",
    "    df.replace([\"?\"],np.nan, inplace = True)\n",
    "    df = df.astype(np.float32)\n",
    "\n",
    "    # imputation\n",
    "    df.fillna(0,inplace=True) # fill nulls with 0\n",
    "\n",
    "    return df \n",
    "\n",
    "df = read_file(\"dataset/allUsers.lcl.csv\")\n",
    "\n",
    "# shuffle the records\n",
    "df_unshuffled = df\n",
    "df = df.sample(frac=1) # shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataframe(df, user=None, val_ratio = 0.25, test_ratio =  0.75, seed = 1):\n",
    "    # split into train, validation and test data using sklearn and return dfs for each\n",
    "    if user!=None:\n",
    "        df = df[df[\"User\"] == user]\n",
    "    if df.shape[0] == 0:\n",
    "        print(f\"Dataframe for user {user} is of shape {df.shape}, no data. Skipping...\")\n",
    "        df = pd.DataFrame()\n",
    "        return df, df, df, df, df, df, df, df, df\n",
    "        \n",
    "    df_train, df_test = train_test_split(df,\n",
    "                                         test_size=test_ratio, \n",
    "                                         random_state = seed)\n",
    "\n",
    "    df_train, df_val  = train_test_split(df_train, \n",
    "                                         test_size=val_ratio,\n",
    "                                         random_state = seed)\n",
    "\n",
    "    # store class and user information (in order)\n",
    "    df_val_class, df_train_class, df_test_class = df_val[\"Class\"], df_train[\"Class\"], df_test[\"Class\"]\n",
    "    df_val_user,  df_train_user,  df_test_user  = df_val[\"User\"],  df_train[\"User\"],  df_test[\"User\"]\n",
    "\n",
    "    # drop the class and user identifier columns from data frame\n",
    "    df_val   = df_val.  drop(df_train.columns[[0,1]], axis=1)\n",
    "    df_train = df_train.drop(df_train.columns[[0,1]], axis=1)\n",
    "    df_test  = df_test. drop(df_test. columns[[0,1]], axis=1)\n",
    "    \n",
    "    return df_val, df_val_class,  df_val_user, \\\n",
    "           df_test, df_test_class, df_test_user, \\\n",
    "           df_train, df_train_class, df_train_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/beta/tutorials/keras/basic_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_class,\n",
    "          val_data, val_class, \n",
    "          epochs=32, verbose=0, return_weights = False, init_seed = 1):\n",
    "    \n",
    "    # same seed value for consistency sake, across all trainings too\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu',\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "        keras.layers.Dense(32, activation='relu',\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "        keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data, \n",
    "        train_class, \n",
    "        epochs = epochs,\n",
    "        verbose = verbose,\n",
    "        validation_data = (val_data, val_class)\n",
    "    )\n",
    "\n",
    "    if return_weights:\n",
    "        return model, history, model.get_weights()\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 0 being trained on the model...\n",
      "User 1 being trained on the model...\n",
      "User 2 being trained on the model...\n",
      "User 3 being trained on the model...\n",
      "Dataframe for user 3 is of shape (0, 38), no data. Skipping...\n",
      "User 4 being trained on the model...\n",
      "User 5 being trained on the model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights = []\n",
    "# for i in range(6):\n",
    "   \n",
    "for i in range(6):\n",
    "    print(f\"User {i} being trained on the model...\")\n",
    "    \n",
    "    df_val, df_val_class,  df_val_user, df_test, df_test_class, df_test_user, df_train, df_train_class, df_train_user = split_dataframe(df,user=i)\n",
    "    if df_val.shape[0]==0:\n",
    "        continue\n",
    "    \n",
    "    model, history, weight = train_model(\n",
    "        df_train.values, \n",
    "        df_train_class.values, \n",
    "        df_val.values, \n",
    "        df_val_class.values,\n",
    "        epochs = 16,\n",
    "        return_weights=True,\n",
    "        init_seed=1\n",
    "    )\n",
    "   \n",
    "    weights.append(weight)\n",
    "#     draw_graphs(history.history)\n",
    "#     print(f\"User {i} done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights are a list of numpy arrays from different layers of the model\n",
    "# weights = [ [user0] [user1] [] [list of numpy arrays of layer weights and biases] [] []]\n",
    "\n",
    "\n",
    "def average_weights(weights):\n",
    "    new_weights = []\n",
    "\n",
    "    for layer_index in range(len(weights[0])):\n",
    "        temp_layer_data = None\n",
    "        for i, weight_user in enumerate(weights):\n",
    "            if temp_layer_data is None:\n",
    "                temp_layer_data = weight_user[layer_index]\n",
    "                continue\n",
    "            temp_layer_data += weight_user[layer_index]\n",
    "        new_weights.append(temp_layer_data/(i+1))\n",
    "    return new_weights\n",
    "\n",
    "new_weights = average_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_users(new_weights, df, users_to_ignore=[3]):\n",
    "    for i in range(len(new_weights)):\n",
    "        if i in users_to_ignore:\n",
    "            continue\n",
    "        df_val, df_val_class,  df_val_user, df_test, df_test_class, df_test_user, df_train, df_train_class, df_train_user = split_dataframe(df,user=i)\n",
    "        print(model.evaluate(df_test.values, df_test_class))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6787/6787 [==============================] - 0s 34us/sample - loss: 23.8358 - acc: 0.2145\n",
      "[23.835775820376753, 0.21452777]\n",
      "3538/3538 [==============================] - 0s 30us/sample - loss: 19.2764 - acc: 0.3143\n",
      "[19.276366829535192, 0.31430188]\n",
      "3385/3385 [==============================] - 0s 33us/sample - loss: 21.9016 - acc: 0.1799\n",
      "[21.901599764013923, 0.17991138]\n",
      "285/285 [==============================] - 0s 50us/sample - loss: 19.4395 - acc: 0.3439\n",
      "[19.439500815408273, 0.34385964]\n",
      "3829/3829 [==============================] - 0s 39us/sample - loss: 18.1693 - acc: 0.3682\n",
      "[18.16926274233619, 0.36824235]\n"
     ]
    }
   ],
   "source": [
    "evaluate_all_users(new_weights, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6787/6787 [==============================] - 0s 35us/sample - loss: 23.8358 - acc: 0.2145\n",
      "[23.835775820376753, 0.21452777]\n",
      "3538/3538 [==============================] - 0s 36us/sample - loss: 19.2764 - acc: 0.3143\n",
      "[19.276366829535192, 0.31430188]\n",
      "3385/3385 [==============================] - 0s 32us/sample - loss: 21.9016 - acc: 0.1799\n",
      "[21.901599764013923, 0.17991138]\n",
      "285/285 [==============================] - 0s 46us/sample - loss: 19.4395 - acc: 0.3439\n",
      "[19.439500815408273, 0.34385964]\n",
      "3829/3829 [==============================] - 0s 37us/sample - loss: 18.1693 - acc: 0.3682\n",
      "[18.16926274233619, 0.36824235]\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(new_weights)\n",
    "evaluate_all_users(new_weights, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
