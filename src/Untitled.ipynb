{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from user import User\n",
    "from average import Average\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# import random\n",
    "# random.seed(SEED)\n",
    "# tf.set_random_seed(SEED)\n",
    "# could need to force keras to not use parallelism, see documentation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def init_model(init_seed=None):\n",
    "    \"\"\"\n",
    "    initialise and return a model \n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "#         keras.layers.Dense(4096, activation='relu',\n",
    "#             kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "#         keras.layers.Dense(1024, activation='relu',\n",
    "#             kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "#         keras.layers.Dense(128, activation='relu',\n",
    "#             kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "        keras.layers.Dense(32, activation='relu',\n",
    "            kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "        keras.layers.Dense(6, activation='softmax',\n",
    "            kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed))\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def read_file(file):\n",
    "    \"\"\"\n",
    "    return 2d df after imputing with 0s\"\"\"\n",
    "\n",
    "    # read data\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # replace the question marks with NaN and then change data type to float 32\n",
    "    df.replace([\"?\"],np.nan, inplace = True)\n",
    "    df = df.astype(np.float32)\n",
    "\n",
    "    # imputation\n",
    "    df.fillna(0,inplace=True) # fill nulls with 0\n",
    "    return df\n",
    "\n",
    "def shuffle_df(df, seed = None):\n",
    "    \"\"\"Shuffle dataframe and reset the index\"\"\"\n",
    "    df = df.take(np.random.RandomState(seed=SEED).permutation(df.shape[0]))\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def acquire_user_data(df, for_user = None, seed = None):\n",
    "    \"\"\"\n",
    "    split the dataframe into train, validation and test splits based on the same seed\n",
    "    Empty dataframes if no data present\n",
    "    \"\"\"\n",
    "    # split into train, validation and test data using sklearn and return dfs for each\n",
    "    if for_user!=None:\n",
    "        df = df[df[\"User\"] == for_user]\n",
    "    if df.shape[0] == 0:\n",
    "        # if no data for the user, then return 9 empty dfs as per the api\n",
    "        # print(f\"Dataframe for user {user} is of shape {df.shape}, no data. Skipping...\")\n",
    "        df = pd.DataFrame()\n",
    "        return df, df\n",
    "    target = df[\"Class\"]\n",
    "\n",
    "    # drop the class and user identifier columns from data frame\n",
    "    df   = df.drop(df.columns[[0,1]], axis=1)\n",
    "    return df, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def init_users(df, averaging_methods, averaging_metric = \"accuracy\", seed = None):\n",
    "    \"\"\"\n",
    "    Requires the DF to contain a \"User\" column giving numeric identity to a user\n",
    "    0 to unique_user_count-1\n",
    "    \n",
    "    Averaging method is a list of methods out of which a random one is selected\n",
    "    \n",
    "    initialise users based on dataframe given and assign random averaging method\n",
    "    to them based on the list passed in.\n",
    "    returns a dictionary of users(key: user object) and a global user object\n",
    "    \"\"\"    \n",
    "    print(\"Initialising User instances...\")\n",
    "    users = dict()\n",
    "    num_users = df[\"User\"].nunique()\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        \n",
    "        user_df, target = acquire_user_data(df = df, for_user=user_id, seed = seed)\n",
    "        \n",
    "        if df.shape[0]==0:\n",
    "            print(f\"User {user_id} has no data, no instance created...\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((user_df.values, target.values))\n",
    "        \n",
    "        users[user_id] = dataset        \n",
    "\n",
    "    print(f\"{len(users.keys())} User datasets created!\")\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising User instances...\n",
      "14 User datasets created!\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "df = read_file(\"../dataset/allUsers.lcl.csv\")\n",
    "df = shuffle_df(df, SEED)\n",
    "\n",
    "averaging_methods = [Average.all,Average.std_dev,Average.weighted_avg]\n",
    "# df.head()\n",
    "\n",
    "users= init_users(df = df, \n",
    "                        averaging_methods = averaging_methods, \n",
    "                        seed = SEED)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Tensor(\"IteratorGetNext_2986:0\", shape=(36,), dtype=float32), Target: Tensor(\"IteratorGetNext_2986:1\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in users[0].take(1):\n",
    "    print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_federated'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cebf43b045da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_federated\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_federated'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# NOTE: If the statement below fails, it means that you are\n",
    "# using an older version of TFF without the high-performance\n",
    "# executor stack. Call `tff.framework.set_default_executor()`\n",
    "# instead to use the default reference runtime.\n",
    "if six.PY3:\n",
    "    tff.framework.set_default_executor(tff.framework.create_local_executor())\n",
    "\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
