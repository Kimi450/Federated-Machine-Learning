{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "b'It works!'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from user import User\n",
    "from average import Average\n",
    "from graphing import *\n",
    "from file_related import *\n",
    " \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "from six.moves import range\n",
    "import six\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# import random\n",
    "# random.seed(SEED)\n",
    "# tf.set_random_seed(SEED)\n",
    "# could need to force keras to not use parallelism, see documentation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "# NOTE: The high-performance executor components used in this tutorial are not\n",
    "# yet included in the released pip package; you may need to compile from source.\n",
    "\n",
    "# NOTE: Jupyter requires a patch to asyncio.\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# NOTE: If the statement below fails, it means that you are\n",
    "# using an older version of TFF without the high-performance\n",
    "# executor stack. Call `tff.framework.set_default_executor()`\n",
    "# instead to use the default reference runtime.\n",
    "if six.PY3:\n",
    "    tff.framework.set_default_executor(tff.framework.create_local_executor())\n",
    "\n",
    "print(tff.federated_computation(lambda: 'It works!')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef printname(name):\\n    print(name)\\nwith h5py.File(file,\"r\") as f:\\n    pass\\n    f.visit(printname)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow_federated.python.simulation import hdf5_client_data\n",
    "# https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/simulation/hdf5_client_data_test.py\n",
    "# https://github.com/tensorflow/federated/blob/v0.11.0/tensorflow_federated/python/simulation/hdf5_client_data.py\n",
    "# http://docs.h5py.org/en/stable/high/group.html#Group.create_dataset\n",
    "# https://stackoverflow.com/questions/55434004/create-a-custom-federated-data-set-in-tensorflow-federated\n",
    "# https://stackoverflow.com/questions/58965488/how-to-create-federated-dataset-from-a-csv-file\n",
    "\n",
    "file = \"dataset.hdf5\"\n",
    "\n",
    "df = read_file(\"../dataset/allUsers.lcl.csv\")\n",
    "NUM_CLIENTS = create_hdf5(df,file,0)\n",
    "\"\"\"\n",
    "def printname(name):\n",
    "    print(name)\n",
    "with h5py.File(file,\"r\") as f:\n",
    "    pass\n",
    "    f.visit(printname)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
    "myclient = hdf5_client_data.HDF5ClientData(file)\n",
    "train = myclient\n",
    "example_dataset = train.create_tf_dataset_for_client(\n",
    "    train.client_ids[0])\n",
    "\n",
    "example_element = iter(example_dataset).next()\n",
    "\n",
    "print(example_element['points'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 20\n",
    "SHUFFLE_BUFFER = 500\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "    def element_fn(element):\n",
    "        return collections.OrderedDict([\n",
    "            ('x', tf.reshape(element['points'], [-1])),\n",
    "            ('y', tf.reshape(element['label'], [1])),\n",
    "        ])\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).map(element_fn).shuffle(\n",
    "      SHUFFLE_BUFFER).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_federated.python.simulation.hdf5_client_data.HDF5ClientData"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2f216fe9803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_example_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sample_batch = tf.nest.map_structure(\n\u001b[1;32m      4\u001b[0m     lambda x: x.numpy(), iter(preprocessed_example_dataset).next())\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_example_dataset = preprocess(example_dataset)\n",
    "\n",
    "sample_batch = tf.nest.map_structure(\n",
    "    lambda x: x.numpy(), iter(preprocessed_example_dataset).next())\n",
    "\n",
    "# sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " <BatchDataset shapes: OrderedDict([(x, (None, 36)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.float32)])>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_federated_data(client_data, client_ids):\n",
    "    return [preprocess(client_data.create_tf_dataset_for_client(x))\n",
    "          for x in client_ids]\n",
    "\n",
    "sample_clients = train.client_ids[0:NUM_CLIENTS]\n",
    "\n",
    "federated_train_data = make_federated_data(train, sample_clients)\n",
    "\n",
    "len(federated_train_data), federated_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(\n",
    "          10, activation=tf.nn.softmax, kernel_initializer='zeros')])\n",
    "\n",
    "    model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "def init_model(init_seed=None):\n",
    "    \"\"\"\n",
    "    initialise and return a model \n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "#         keras.layers.Dense(4096, activation='relu',\n",
    "#             kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "#         keras.layers.Dense(1024, activation='relu',\n",
    "#             kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "#         keras.layers.Dense(128, activation='relu',\n",
    "#             kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "        keras.layers.Dense(32, activation='relu',\n",
    "            kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed)),\n",
    "        keras.layers.Dense(6, activation='softmax',\n",
    "            kernel_initializer=keras.initializers.glorot_uniform(seed=init_seed))\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "#         metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    keras_model = init_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kts1/proj/env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kts1/proj/env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> <model=<trainable=<sequential/dense/kernel=float32[36,32],sequential/dense/bias=float32[32],sequential/dense_1/kernel=float32[32,6],sequential/dense_1/bias=float32[6]>,non_trainable=<>>,optimizer_state=<int64>,delta_aggregate_state=<>,model_broadcast_state=<>>@SERVER)\n"
     ]
    }
   ],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)\n",
    "print(str(iterative_process.initialize.type_signature))\n",
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  1, metrics=<sparse_categorical_accuracy=0.9383515119552612,loss=0.5496358871459961>\n",
      "round  2, metrics=<sparse_categorical_accuracy=0.9629532694816589,loss=0.18527714908123016>\n",
      "round  3, metrics=<sparse_categorical_accuracy=0.9692147970199585,loss=0.13197511434555054>\n",
      "round  4, metrics=<sparse_categorical_accuracy=0.9709460139274597,loss=0.11831852793693542>\n",
      "round  5, metrics=<sparse_categorical_accuracy=0.9749155044555664,loss=0.10432572662830353>\n",
      "round  6, metrics=<sparse_categorical_accuracy=0.9751062989234924,loss=0.10409539192914963>\n",
      "round  7, metrics=<sparse_categorical_accuracy=0.9757708311080933,loss=0.09923163801431656>\n",
      "round  8, metrics=<sparse_categorical_accuracy=0.975578784942627,loss=0.09917626529932022>\n",
      "round  9, metrics=<sparse_categorical_accuracy=0.9780449867248535,loss=0.09046626836061478>\n",
      "round 10, metrics=<sparse_categorical_accuracy=0.9793152213096619,loss=0.0887361615896225>\n"
     ]
    }
   ],
   "source": [
    "state, metrics = iterative_process.next(state, federated_train_data)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "NUM_ROUNDS = 11\n",
    "for round_num in range(2, NUM_ROUNDS):\n",
    "    state, metrics = iterative_process.next(state, federated_train_data)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
