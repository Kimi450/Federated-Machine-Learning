\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage[linesnumbered,noline,titlenumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{extsizes}
\usepackage{tocloft}
\usepackage{caption}
%\usepackage{url}
\usepackage{amsmath}
\usepackage[all]{nowidow}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\newcommand{\comment}[1]{}

\SetKwFunction{FFedCore}{federated\_learning\_core}%
\SetKwFunction{FFedCoreUser}{user}%
\SetKwFunction{FFedCoreServer}{central\_agent}%
\SetKwFunction{FAvg}{average}%
\SetKwFunction{FDataSplit}{global\_user\_data\_init}%
\SetKwFunction{FDataSepCsv}{separate\_csv}%
\SetKwFunction{FDataSepDir}{separate\_dir}%


\SetStartEndCondition{ }{}{}%
\SetKwProg{Fn}{def}{\string:}{}
\SetKwFunction{Range}{range}%%
\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
\SetKwFor{While}{while}{:}{fintq}%
\newcommand{\forcond}[2]{#1 \KwTo #2}
\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

\usepackage[backend=biber, style=IEEE, sorting=nty]{biblatex}
\addbibresource{resources/references.bib}

\begin{document}
\makeatletter

% title page
\begin{titlepage}
	\begin{center}
		\line(1,0){320}\\				
		[0.32cm]
		\huge{\bfseries Federated Machine Learning}\\
		[0.16cm]		
		\line(1,0){320}\\
		[0.64cm]		
		\textsc{\LARGE A research project}\\
		[10cm]
	\end{center}
	\begin{center}
		\textsc{\large Karan Samani}\\
		\textsc{\large 1161081087}\\
		\textsc{\large Final Year Project BSc. Computer Science (Hons.)}\\
		\textsc{\large Supervisor: Dr. Derek Bridge}\\
		\textsc{\large Department of Computer Science}\\
		\textsc{\large University College Cork}\\
		\textsc{\large \@date}
	\end{center}
\end{titlepage}
\cleardoublepage

% Front matter
\pagenumbering{roman}
\section*{Abstract}
\addcontentsline{toc}{section}{\numberline{}Declaration}
In this report we will see the motivation behind the need for a federated process for machine learning, a process that preserves privacy. A brief overview of how machine learning and neural networks work will be provided, which is required to understand how the federated learning process works. After doing so, we will be moving onto the implementing the federated approach proposed by Google.
\\\\
Google's approach will be implemented from scratch along with an implementation using Tensorflow's Federated framework. The latter being used as a sanity check to confirm that the implementation from scratch is indeed correct, doing so by running a few experiments and comparing results. After doing so, we will move on to exploring extended approaches to the federated learning idea based on a weighted and selective approach on a global level and a user level.
\\\\ 
Once the implementation has been explored, we will run several experiments to compare the approaches. These include traditional machine learning, federated learning and the several extended approaches that were explored.
\clearpage
\section*{Declaration of Originality}
\addcontentsline{toc}{section}{\numberline{}Abstract}
%\begin{center}
%\large \textbf{Declaration of Originality}
%\end{center}
In signing this declaration, you are conforming, in writing, that the submitted work is entirely your own original work, except where clearly attributed otherwise, and that it has not been submitted partly or wholly for any other educational award.
\\\\
I hereby declare that:
\begin{itemize}
  \item this is all my own work, unless clearly indicated otherwise, with full and proper accreditation;
  \item with respect to my own work: none of it has been submitted at any educational institution contributing in any way to an educational award;
  \item with respect to another’s work: all text, diagrams, code, or ideas, whether verbatim, paraphrased or otherwise modified or adapted, have been duly attributed to the source in a scholarly manner, whether from books, papers, lecture notes or any other student’s work, whether published or unpublished, electronically or in print.
\end{itemize}
\noindent Signed:\dotfill
\\\\
Date:\dotfill
\clearpage

\section*{Acknowledgements}
Thanks to Dr. Derek Bridge for being my supervisor and dealing with me over the last few months, pushing me to my limits and supporting me throughout the year. Ever since first year, I wanted to do my final year project with him and the experience of doing so has lived up to my expectations. 
\\\\
Thanks to the Alexander Baran-Harper's channel on YouTube where I learnt how to use use LaTeX.
\addcontentsline{toc}{section}{\numberline{}Acknowledgements}
\clearpage


% Table of content stuff
\tableofcontents
\thispagestyle{empty}
\cleardoublepage

% list of figures
\listoffigures
\addcontentsline{toc}{section}{\numberline{}List of Figures}
\cleardoublepage


% project work
\setcounter{page}{1}
\pagenumbering{arabic}





\section{Introduction}
\subsection{Background}
Modern day edge devices have a wealth of data on them and have more than enough computation power to run complex calculations on them with ease. These devices can range from a personal computer to a smart phone. In a world where data is power, access to the data on these devices is highly advantageous.
\\\\
Artificial Intelligence (AI) can be described as the ability for a system to show “intelligence”. Intelligence, as Dr. Derek Bridge put it, is the ability for a system to act autonomously and rationally when faced with disorder, uncertainty, imprecision and intractability. Machine Learning is a branch of AI which is based on the idea of creating a model that recognises patterns from data to be able to solve problems. Neural Networks (Section~\ref{sec:neuralnets}) are an example of machine learning with Deep Learning being a subset of neural networks where the models used have more layers in them. This division can be visualised in Figure~\ref{fig:ai}. To be able to do so effectively, one must have access to a lot of data. More data equates to a higher probability of having a more robust and better overall model. If a model can look and learn from more data, the chances are that it can generalise well, and that is the ideal goal. 
\begin{figure}[H]
	\centering
	\includegraphics[width=6cm]{resources/ai.png}
	\caption{High level visual representation of what fields there are in the study of AI.}
	\label{fig:ai}
\end{figure}
\noindent The solution to having well trained models seems straight forward. We should use the vast amounts data from the edge devices to train a model that, in theory, should be fairly accurate. To do so, the users must give their data to a server so that the server can then train the model on the data that was just provided to it by the edge users. This trained model can then be used by everyone to predict unseen samples. But there are some more complications. Users may not want to share their data but still want the benefits of having a model trained on everyone's data. 
\subsection{Motivation}
Artificial Intelligence, specifically Machine Learning, is an idea that is taking the world by storm. A piece of software that would make machines autonomous. It was supposed to be better in every way, never getting tired, reaching at least the same competency levels as humans and quite possibly even better. It was hyped up to the point where the media started talking about AI putting people out of a job and eventually taking over the world. They even made far fetched references to the popular movie series Terminator.
\\\\
Needless to say, this is not accurate. The idea of a general purpose AI, formally called Artificial General Intelligence, is no where near attainable. Current AI applications are good at specific tasks, and only those tasks because they have a narrow scope. Even with their narrow scope there are more topics that need to be addressed, such as the security issues that arise with better AI systems. One can use AI to create cyberweapons that can be used for hacking and spreading misinformation. Along with cyberwarfare, AI can be used to make traditional weapons more lethal. For instance, drones are now being used to target specific areas (and people) using ideas like image classification. At first glance this seems like a good idea as it should result in less casualties. But, AI systems are not 100\% accurate so they could lead to an accidental strike. And if someone is able to hack into the system, they can make the drones target literally anything and anyone. Even in the right hands this technology can lead to disasters, let alone the wrong hands. Then there is the privacy aspect as well where people may not want to share their potentially sensitive private data under the fear that it can be misused. The idea of privacy will be the focus of this project.
\subsection{Privacy concerns}
There are numerous examples where people may not want to share their personal data. For instance, for the training of a model that deals with predictive text, the input data would require essentially everything that a user may type into their device. It is pretty obvious to see why some people may not want to share the messages and other content that they type on their devices. It is a clear invasion of their privacy.
\\\\
Another application could be training on images for classification purposes. People may not want to share images, which may include sensitive images, that they have stored on their devices with a third party. This can be extended to an even more sensitive topic of medical imaging where people may not want to share something like the X-Rays of their bodies.
\\\\
In general, people are sceptical of sharing personal data. But there is still the need to train a model that has had exposure to as much data as possible. 
\subsection{Outline of the report}
The need for privacy was the motivation behind the idea of Federated Learning which was an idea proposed by Google in 2016~\cite{konen2016federated}. The idea of not having to share your data with someone else and yet still have the benefits of having a model that has exposure to their data will be the main point of interest in this report. 
\\\\
To start off, a brief overview of machine learning and neural networks will be provided in Sections~\ref{subsec:ml} and~\ref{sec:neuralnets} respectively. Then in Section~\ref{subsec:fedml} we will see how Google describe federated learning before discussing the design and implementation in Section~\ref{subsec:fedml-core}. Following Google's approach, several extended ideas based on weighted and selective approaches in a central and peer-to-peer environment will be discussed and their implementation explained in Section~\ref{sec:ext-ideas}. Along with that, outputs from the experiments to show how the extended ideas compare with Google's approach of federated learning and the traditional approach of machine learning in this context will be explained in Section~\ref{sec:experiments}. 
\section{Literature Review}
\subsection{Machine learning}\label{subsec:ml}
Machine learning is a very broad field which includes a lot of different learning methodologies. Learning can take place in a supervised context where the dataset is labelled, or in an unsupervised context where the data is not labelled. 
\\\\
In unsupervised learning, the only input data are the features and there are no input-output mappings. The aim is to find structure within the dataset and can also be useful in finding anomalies in the dataset. The most well known algorithm for this purpose is k-means++ clustering.
\\\\
In supervised learning, the task is to learn a function that maps an input to an output, based on sample input-output pairs. In this project, the focus is on supervised learning where the aim is not to find structure within a dataset but rather trying to learn how to map the input data to a desired output. There are quite a few methods of finding the right function that fits the dataset, ranging from simple functions to very complicated functions.
\\\\
One of the simplest approaches for supervised learning is called Linear Regression, which aims to solve regression problems. The data that is provided to it are pairs consisting of input features (as a vector) and the desired output. Based on the set of input pairs provided, linear regression tries to find a linear function, which is a set of coefficients $\beta$ for all the features, that fits the dataset well. The set of input pairs provided is called the training set. To find the best possible function to fit the data, the idea of a loss function is used. This essentially says how distant the predictions are from the desired output. Loss in this case is usually mean squared error (MSE). The error $e$, is the difference between the actual value and the predicted value.
$$\mbox{MSE} = \frac{1}{n}\sum_{t=1}^{n}e_t^2$$
The values of $\beta$ that fit the dataset the best would be the one with the lowest value for MSE on the training data. A naive way to solve this problem would be to iterate over all the infinitely many $\beta$s and run predictions on the $n$ samples in the training set to give an output. We then use use the predicted output and the desired output to calculate the loss values for all the $\beta$ combinations. The $\beta$ values that give the lowest MSE value (loss) would be chosen as the solution. But there are more sophisticated methods of solving this such as gradient descent. The latter can intuitively be thought as taking, usually small, steps in the direction that reduces the loss. 
\\\\
Logistic Regression follows the same basic principle as linear regression but is used for classification purposes instead. Classification problems are about predicting if a sample belongs to a certain class or not. The input data for this is similar to linear regression with it being a pair of input features (as a vector) but the desired output being a label representing a class of objects (like ``dog''). Logistic regression still works off of building linear models using $\beta$ under the hood and predicts numbers that are probabilities of a certain input being part of a certain class. For the prediction, input features are passed through a sigmoid function $\sigma$ (also called the logit function) which outputs a number between 0 and 1, lets call this $h_\beta$. 
$$\sigma(z) = \frac{1}{1 + e^{(-z)}}$$
$$h_\beta(x) = \sigma(x\beta)$$
These numbers are interpreted as the probability of the input being part of a class, usually the positive class (a class which requires action and is labelled as 1). Based on the probability, the input is classified to a class $\hat{y}$.
$$\hat{y} = \left\{ 
    \begin{array}{ll} 
      0 & \mbox{if \textit{Prob}}(\hat{y} = 1 | x) < 0.5 \\
      1 & \mbox{if \textit{Prob}}(\hat{y} = 1 | x) \geq 0.5
    \end{array}
  \right.
$$
The idea of reducing the loss still applies, but a more complicated loss function is used in this case. Linear regression and logistic regression are both based off of a linear function so they cannot deal with complex data very well. Decision trees are an alternative that can better fit complex datasets and can be used for both regression and classification problems. They are more intelligible compared to other approaches. At a very high level, the structure of the tree dictates what path a sample input should take. A very simple decision tree can be seen in Figure~\ref{fig:decisiontree}. The inner nodes in the tree split the data based on conditions and the leafs represent the decisions made on the samples. A different loss function is used to optimise the answer.
\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{resources/decisiontree.png}
	\caption{A very simple decision tree~\cite{phdthesis}.}
\end{figure}\label{fig:decisiontree}
\noindent Neural networks have become the go to solution in recent times for pretty much all problems. They can cater to a wide range of problems, including very complex problems such as image classification, image localisation and natural language processing. They generally perform pretty well on those tasks too. There are also ways in which privacy concerns can be addressed when using neural networks. This is why they will be the only thing we use in this project.  
\subsection{Neural Networks}\label{sec:neuralnets}
Neural networks, as seen Figure~\ref{fig:ai}, are a subset of machine learning. Neural networks are not a new idea, they have been around for decades. But they only really took off in recent years with the emergence of better and more affordable hardware. The basic idea behind the workings of a neural network are quite straight forward. A model is defined and data is passed through it to make predictions. Then, based on the loss, some adjustments are made to the parameters learnt. This whole process is repeated by iterating over the dataset a set number of times during the training process. To start off, the idea of a neuron must be explained which are the basic building blocks of a neural network.
\\\\
\subsection{Neurons}
A neuron computes a weighted sum of its inputs, passes it through a function (called the activation function) and outputs a value to be used later. The inputs received are from either the input layer neurons or the hidden layer neurons. The activation function used below is a step function that outputs a $1$ if the weighted sum is more than a certain value ($0$ in this case), otherwise it outputs a $0$. The weight $w_0$ for the input data point labelled $1$ in Figure \ref{fig:neuron} is used to represent a bias. Because the input is $1$, multiplying it with a weight $w_0$ is guaranteed to give a value which will act as a number that is always used in the summing process later before the value is passed into the activation function. 
\begin{figure}[H]
	\centering
	\includegraphics[width=6cm]{resources/neuron.png}
	\caption{Major components of a neuron~\cite{web:neuron}.}
	\label{fig:neuron}
\end{figure}
\noindent Note that, for brevity, the weighted sum can be written in a vectorised format. The weighted sum also includes $w_0$ which represents the bias.
$$w \cdot x \equiv \sum_j w_j x_j$$
$$  \mbox{output} = \left\{ 
    \begin{array}{ll} 
      0 & \mbox{if } w\cdot x \leq 0 \\
      1 & \mbox{if } w\cdot x > 0
    \end{array}
  \right.
$$
The activation function can be swapped out from the step function that was being used earlier with some other activation function such as ReLU (Rectified Logic Unit), sigmoid function, etc. ReLU takes the max of either 0 or the weighted sum and uses that as its output. This can be seen in equation~\ref{eq:relu}.
\begin{eqnarray}\label{eq:relu}
  \mbox{output}&=& \mbox{max}\left\{0,w\cdot x\right\}
\end{eqnarray}
\subsection{Architecture}\label{subsec:arch}
In a neural network, there are layers of such neurons. These are usually broken down in three parts, the input layer, the hidden layer(s) and then the output layer. The input layer is not actually a layer of neurons but rather just a representation of the input data as a layer that connects to the hidden layers. The hidden layers can contain any number of layers with any number of neurons for the layers.
\\\\
After the hidden layers is the output layer. This layer is responsible for outputting the predictions. The output layer usually has its own activation function which depends on the application, i.e., if it is a classification problem or a regression problem. Since this project focuses on multi-class classification problems, we will be using the softmax activation function for the output layer. 
\\\\
% start here-------------------
The layered structure described above is called the architecture of the model. Some of the common used architectures are densely connected layers (Section~\ref{subsubsec:dense}) and convolutional layers (Section~\ref{subsubsec:conv}). More information on the aforementioned and more architectures can be found in the book about Deep Learning by F. Chollet~\cite{deeplearning}.
\subsubsection{Dense}\label{subsubsec:dense}
These are one of the most straight forward architectures and are generally used as the output layer of most neural networks. They are quite useful when placed as the last few layers as well, especially for classification purposes. In these, all the neurons are connected to every neuron in the subsequent layer. The input for these layers are flattened data, which can be thought of as a list of input data where nested lists are not allowed. An example can be seen in Figure \ref{fig:densenet}. 
\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{resources/densenet.png}
	\caption{Basic dense neural network~\cite{bre2018prediction}}
	\label{fig:densenet}
\end{figure}
\subsubsection{Convolutional}\label{subsubsec:conv}
These are more complicated than the previously mentioned dense layers. The input data here is structured and not flattened. Their basic idea is to find patterns in the input data and make them more abstract as the layer count increases. They indicate the presence of certain shapes. The more common use for convolutional layers is in image classification.
\\\\
A convolutional layer has a window (called the kernel) that is used to look over the input data and recognise patterns localised in that window. Every layer has a number of sub-layers which can be thought of as the number of patterns that the layer is trying to recognise and they are called the feature maps of the layer. For example if the depth is 3, the convolutional layer has 3 feature maps and will look at 3 kinds of patterns. The neurons in the following convolutional layer are connected to every neuron in the window of the previous layer, including the sub-layers as well. The set of weights that connect a neuron to the sub-layer neurons in the following layer are the same within the window. Figure \ref{fig:convnet} does a good job of giving a visual representation for the same.
\begin{figure}[H]
	\centering
	\includegraphics[width=8cm]{resources/convnet.png}
	\caption{How a convolution works~\cite{deeplearning}.}%page 125, chapter 5
	\label{fig:convnet}
\end{figure}
\noindent Convolutional layers are often followed by some maxpooling layers, which reduce the output size from the convolutional layers. This can be done for many reasons, such as saving memory and making the computations required less intensive. At the end of a series of convolutional layers, there is a series of dense layers that are used to give the predicted output(s). 
\clearpage
\subsection{Training}\label{subsec:training}
For a neural network to work, the weights with which the neurons are connected need to be tweaked and the process of doing so is called training the model. The model is trained on a training set, which is a subset of the whole dataset. Additionally, a validation set can be provided to see how the model performs on unseen data that is not the testing data. The progress is seen by comparing metrics for the model on both the training data and validation data over the course of the training. This is done to avoid leakage between the testing data and the training data. We will now go through the high level algorithm with which neural networks are trained. More information on this and the idea of neural networks can be found in the (online) book ``Neural Networks and Deep Learning, Michael A. Nielsen''~\cite{neuralnets} and the book ``Deep Learning with Python, Francois Chollet''~\cite{deeplearning}.
\\\\
The model is initialised with random weights before training begins. After that, the training data is then through the model to make predictions. These predictions are then compared with the actual values, and the loss is calculated using a loss function. For regression, mse is used and for classification sparse categorical crossentropy is used. The details in which they work are not required, but the idea of finding out how far off the predictions were from the actual value still holds. Using an methods like gradient descent or Adam (based on gradient descent and other methods, an algorithm called back propagation tweaks the weights in a way that would hopefully reduce the loss. These updates can be made based on every example, batches of examples or the entire dataset as a whole. Generally, the batched approach is taken and the entire dataset is broken down into batches of training data. 
\\\\
This whole process of predicting, calculating loss and tweaking the weights can be done several times by iterating over the whole training set several times. The number of times that the entire training set has been gone over is called the number of epochs that the network was trained on. This needs to be tweaked manually as it could lead to under-fitting or over-fitting the model. Which means, the model is too general or too specific, respectively, to be useful in actual usage.
\comment{\subsection{Differential Privacy (MIGHT OMIT THIS)}
linkage attacks, netflix and imdb data to identify
only good for large dataset, else bad 
	complex
	easier to do real data and anonymise 

deidentify data (outliers, fields)
 	
want to decouple learning about an individual and learning about the population

analysis output is not dependant on a particular individual and will be the sameif they are not incuded

adjacent dataset
	n+1 and n sized dataset, algo m
}
\subsection{Federated Machine Learning}\label{subsec:fedml}
Traditionally in ML, a server would have access to all the data. We call this server the central agent $C$. The data on the central agent is collected from a set of upto $n$ edge users which we call $U$. Each edge user i of U, represented as $U_i$, sends their data to the central agent. User $i$'s data is represented as $D_i$. Using all the data $C$ has access to, $D = \bigcup_{i=1}^{n} D_i$, it would train a single model $M$ which then anyone could then access to make predictions. 
\\\\
In Federated ML, $D_i$ remains with $U_i$ and instead only the weights of the user's model are shared. This means that at any give point, $C$ does not have access to any data $D_i$ from user $U_i$. By doing so, the privacy of the user's data is maintained. The training process in federated learning is performed on $U_i$'s device itself. To be able do so, an identical copy of $M$ is sent from $C$ to every participating $U_i$. Every $U_i$ then treats the model $M$ that they received as $M_i$. This means that every $M_i$ has the same architecture and the same randomly generated initial weights $W_i$. The weights are associated to the links that connect the neurons of the neural network as seen in Section~\ref{sec:neuralnets}.
\\\\
Once $U_i$ receives $M_i$, it can start training its $M_i$ on its local data $D_i$. The training process for $M_i$ generally starts based on certain conditions being met such as the device being plugged in for charging, WiFi connection, usage, etc. After the local training process for $U_i$ has been completed, $M_i$ would have new weights $W_i$. These weights would have been learnt and set such that they fit the user's $D_i$ relatively well. User $U_i$ then sends its learnt weights $W_i$ to $C$ for the averaging process to take place. The algorithm for the user side operations can be seen in Algorithm~\ref{algo:fedcore_user}. Instead of sending all the weights, an alternative would be to send only the changes made to the weights, which is what Google does.
\begin{center}\begin{algorithm}[H]
\texttt{\Fn{\FFedCoreUser{new\_weights}}{
	\If{new\_weights != None}{
		$M_i$.set\_weights(new\_weights)\\	
	}
	\For{\forcond{$e$}{$local\_epochs$}}{
		$M_i$.train()
	}
	$M_i$.send\_weights\_to\_server()
	return $M_i$.weights
	\caption{User side processing}\label{algo:fedcore_user}
}
}
\end{algorithm}\end{center}
\noindent When $C$ receives a set of upto $n$ weights $W_i$ from all the participating users, it uses them to calculate the average of the set of weights $W_{avg}$. The averaged weights $W_{avg}$ are then sent from $C$ to every $U_i$ in $U$. This can be seen in Algorithm~\ref{algo:fedcore_server}.
\begin{center}\begin{algorithm}[H]
\texttt{\Fn{\FFedCoreServer{$set_of_weights$}}{%\bigcup_{i=1}^{n}W_i
	$W_{avg}$ = average($set_of_weights$)\label{algo:fedcore_server:line:avg}\\
    send\_to\_all\_users($W_{avg}$)
    \caption{Server side processing}\label{algo:fedcore_server}
}
}
\end{algorithm}\end{center}
\noindent The averaging process is the reason why we need to use the same architecture for all models $M_i$. Without the same architecture, the shape for every $W_i$ would be different and therefore we would not be able to calculate an averaged $W_{avg}$ that would be compatible with every $M_i$. The averaging mentioned in in Algorithm~\ref{algo:fedcore_server} line~\ref{algo:fedcore_server:line:avg} is calculated by summing all received $W_i$ and dividing by the number of $W_i$ received, as seen in equation~\ref{eq:avg}.
\begin{equation}\label{eq:avg}
	W_{avg} = \frac{\sum_{i=1}^{n} W_i}{n}
\end{equation}
\noindent After receiving $W_{avg}$ from $C$, every $U_i$ replaces the weights $W_i$ for their $M_i$ with the new averaged weights $W_{avg}$. When the users have set the weights of their model $M_i$ to be $W_{avg}$, they can start the training process again. This back and forth of training, averaging and training again can take place several times. Every time $U_i$ trains their $M_i$ and shares their $W_i$ with $C$ to receive $W_{avg}$, it is called a $round$ of federated learning. After a few rounds, the resulting metrics can be pretty close to the metrics obtained with the traditional ML approach. Although, it must be noted that depending on the context the results may vary a lot and may require some changes as well. We will witness this in the experimentation section (Section~\ref{sec:experiments}) later in this report. 
\\\\
Algorithm~\ref{algo:fedcore} and Figure~\ref{fig:fedcore} provide a high level representation of the federated learning process. It starts off with $C$ sending the initial model to the all the users. This model is the same $M$ that was initialised with random weights in the traditional approach. It is provided to the algorithm as an input. After the users receive the model, for every round of federated learning, all the users set their weights to the new averaged weights (if provided), run local training and then share their current weights with $C$. $C$ would then calculate the new averaged weights and broadcast it to every $U_i$ and the whole process takes place again for a set number of rounds. The calls made in lines~\ref{algo:fedcore:line:user} and~\ref{algo:fedcore:line:server} are calls to algorithms~\ref{algo:fedcore_user} and~\ref{algo:fedcore_server} respectively.
\begin{center}\begin{algorithm}[H]
\Fn{\FFedCore{model}}{
	central\_agent.send\_to\_users(model)\\
    new\_weights = None\\
    \For{\forcond{round}{$rounds$}}{
	    weights = [ ]\\
        \For{\forcond{user}{$users$}}{
            user\_weights = central\_agent(new\_weights)\\\label{algo:fedcore:line:user}
            weights.add(user\_weights)
        }
        new\_weights = server(weights)\label{algo:fedcore:line:server}
    }
  \caption{Federated Learning Core overall algorithm}\label{algo:fedcore}
}
\end{algorithm}\end{center}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{resources/fedml.png}
	\caption{Federated learning.}
	\label{fig:fedcore}
\end{figure}
\subsubsection{Benefits}
Federated machine learning has a few benefits. Firstly and most importantly, all the training takes places on the only the edge devices. This means that the users do not have to share their data with anyone. The only data they share are the parameters learnt from the training process that took place on their local data. And it is impossible to recreate the original data from just the parameters. Add to that the idea of Secure Aggregation~\cite{secagg} and one can very confidently say that the idea of privacy is held up to the highest standard. Secure aggregation is where the data is aggregated in stages. Instead of all averaging being done at the server where some data can possibly be reverse engineered, there are intermediary steps where the data is averaged and then the central agent finally averages those averages. Sometimes, noise is also added during this process. But we will not be focussing on secure aggregation in this project. 
\\\\
Secondly, the fact that all the training runs on edge devices means that there is no need for an investment in building a large training infrastructure by a company. The edge devices will do all the hard work of the model training and share the results which a central agent would then use quite trivially. So this is a better use of resources as a whole as idle devices would not just stay idle and would take part in the training process.
\subsubsection{Drawbacks}
As mentioned before, the federated approach can lead to similar performance as the the traditional approach. But this depends on the distribution of the data as we will see in later sections (\ref{subsec:imageset}. If the dataset amongst all the users is very similar, then the overall result of the federated process can be very similar to the traditional approach. But if the users have skewed datasets, as is often the case in real life, the traditional approach is generally better. It is a trade-off of privacy vs. model performance that must be decided upon when deciding between the federated approach and the traditional approach respectively.
\clearpage

\section{Core Design}
In this section, the design and implementation aspect of the basic approaches will be discussed before moving on to discussing the extended ideas explored in this project. 
\subsection{Non-Federated Learning}\label{subsec:global-approach}
Before talking about federated learning, we first need to set in place the traditional machine learning approach. For this, the idea of an imaginary user is utilised. We refer to this user as the $global\_user$. The $global\_user$ assumes that we have access to data from every user. We set the metrics from this approach as \textit{the} benchmark to beat. If any federated approach can be close to these metrics, then we can say that we can preserve privacy whilst maintaining a similar level of performance.
\subsubsection{Design}
This approach is quite straight forward in terms of what is to be done. The data is readily available to be used, so the first first step is to pass the training data into the model for training. The second one is to record how well the model performs on the training and validation data for every epoch of training.
%approach involves finding a way to split the data, selecting a model, passing the data to the model for training and recording the metrics to analyse the performance of the model. 
\\\\
With the dataset readily available for use, we need to select a model that fits the dataset well. We will look at how to find a dataset specific model in Section~\ref{sec:experiments} where we perform experiments on different datasets. Given that the data and model are now ready to be used, we can pass the training data into the model to train it. The training process described in Section~\ref{subsec:training} is carried out and the model is trained on the training data for a number of epochs. To ensure that we can review the progress of the model, we also make sure that we evaluate the model between every epoch of training on the validation and testing data. This can then be plotted on a metric versus epoch graph to visualise the model's performance over time. For the final performance score of the model, we can evaluate the model after the training process has been completed on the completely unseen test data.
\subsubsection{Implementation}
With the approach being straight forward, its implementation is quite trivial. To be able to discuss the implementation, an overview of the technologies used in this project is required. The language of choice for this project was Python and the biggest reason for choosing it was the Keras library for deep learning. In this project, it uses with TensorFlow as its back-end to perform calculations. Keras provides a user friendly API to use Tensorflow to build neural networks with ease in Python. We chose to use TensorFlow as Keras' back-end because we will be using TensorFlow Federated (TFF) in Section~\ref{subsec:tff} as a sanity check to confirm that our implementation of federated learning matches that of Google. TFF provides a framework which allows people to build a federated learning system without having to deal with the low level details. Another library used extensively was Matplotlib. Matplotlib is a Python library used for plotting graphs which. It allowed us to review the learning progress of the neural network and be able to compare the performance of different approaches explored in this project. Pandas and numpy were two more extensively used libraries that were used for data representation throughout the project.
\\\\
The model $M$ that is used here is initialised in Keras. The initialisation process is essentially selecting an architecture that works well on the dataset and then compiling it for further use, like training. Keras models require the input for the training process to be data in one of either two formats, pandas DataFrames or numpy arrays. We choose the latter as the way to store the data that we have to make it easier to pass data into the model for training and evaluation. Keras also requires the features of the data to be separate from the label associated with a sample. The low level control on the numpy arrays also allows us to perform more specific tasks, such as this separation of features and labels, on the dataset as and when required.
\\\\
To train the model, we use the API that Keras provides for its models. A method called \texttt{fit} is associated to the model object which is used to train the model. The arguments passed into the method are the training data, validation data, epochs and the batch size. The training data is used to train the model. The validation data is used to gain some insight into how the model performs on data that is not the training data. Epochs is the number which specifies the number of times the training process iterates over the entire training data. The batch size dictates the number of samples in the training data that must be processed before changes to the weights of $M$ can be made. A simple call to this method on the model object will result in our model being trained on the training data.
\\\\
We also need to record the performance of the model over time to be able to plot graphs using matplotlib. Keras, whilst training, maintains a history of the metrics obtained between epochs on the model object. Between epochs, the model is evaluated on the training and validation data, and the result is stored in a dictionary in the model object. This dictionary can be accessed to get what is essentially the progress report for the model on the validation and training data. We use this data to plot the graphs to see how well the model is performing. 
\\\\
\subsection{Federated Learning}\label{subsec:fedml-core}
With the non-federated learning approach in place, we can discuss the federated approach in a similar way. This approach is exactly as explained before in Section~\ref{sec:fedml} and as such, this section will not include a lot of details which were explained previously. 
\subsubsection{Design}
In any federated approach explored, there is an obvious need to simulate users that take part in the process. But there is no point in  implementing a network of users when the idea of users can be simulated on just one machine. These simulated users have one main goal, they must not be able to access data from other users. We needed to come up with a way to simulate users who only have access to their local data and their local model. In other words, a way where only $U_i$ can access $M_i$ and $D_i$. To do so, we chose an objected oriented approach where the users would be represented as objects $U_i$. These objects would contain all the information relevant to a given user and nothing more. This means that user object $U_i$ would have access to $M_i$ and $D_i$. It is to be noted that for a given dataset, all user models $M_i$ are the same as model $M$ which was selected for the $global_user$. 
\\\\
The core federated learning algorithm used here is the same as Google's algorithm~\ref{algo:fedcore} described in Section~\ref{subsec:fedml}. But it's design was altered just a little bit to allow us to review the progress of the federated learning process and accommodate our for choice of not representing $C$ as a distinct user. 
\\\\
The decision to not represent $C$ as an object was taken because its functionality could be incorporated in the logic of the algorithm itself. This is done by replacing line~\ref{algo:fedcore:line:server} in algorithm~\ref{algo:fedcore} by line~\ref{algo:fedcore_server:line:avg} in algorithm~\ref{algo:fedcore_server}. As for being being able to review progress at the end, we had to store evaluation metrics of the model on the test data before and after the local training process. These evaluations were done during every round of federated training. We call these evaluations $pre\_fit$ and $post\_fit$ evaluations respectively. The $pre\_fit$ evaluations allow us to see the model's performance with the averaged weights and the $post\_fit$ evaluations allow us to see the model's performance with the weights after local training is performed. This helps us see how well the averaged weights and post training weights fit an arbitrary user $U_i$'s test data $D_i$. 
\\\\
We augment algorithms~\ref{algo:fedcore} and~\ref{algo:fedcore_user} to reflect these changes which results in algorithms~\ref{algo:fedcentral} and~\ref{algo:fedcentral_user}. The averaging in line~\ref{algo:fedcentral:avg} still uses equation~\ref{eq:avg}.
\begin{center}\begin{algorithm}[H]
\texttt{\Fn{\FFedCoreUser{new\_weights}}{
	\If{new\_weights != None}{
		$M_i$.set\_weights(new\_weights)\\	
	}
    evaluation = user.evaluate\_model()\\
	user.add\_pre\_fit\_evaluation(evaluation)\\
	\For{\forcond{$e$}{$local\_epochs$}}{
		$M_i$.train()
	}
    evaluation = user.evaluate\_model()\\
    user.add\_post\_fit\_evaluation(evaluation)\\ 
   	$M_i$.send\_weights\_to\_server()
	return $M_i$.weights
	\caption{User side processing}\label{algo:fedcentral_user}
}
}
\end{algorithm}\end{center}
\begin{center}\begin{algorithm}[H]
\Fn{\FFedCore{model}}{
    send\_to\_users(model)\\
    new\_weights = None\\
    \For{\forcond{round}{$rounds$}}{
	    weights = [ ]\\
        \For{\forcond{user}{$users$}}{
           user\_weights = user(new\_weights)\\          
           weights.append(user\_weights)\\
        }
        new\_weights = average(weights)\label{algo:fedcentral:avg}
    }
  \caption{Federated Learning}\label{algo:fedcentral}
}
\end{algorithm}\end{center}
\noindent The algorithm starts off by sending the model $M$ to every user. Each user then refers to their model as $M_i$. Then, before the model is trained by the user, it is evaluated on the users test data and that information stored in the collection of $pre\_fit$ evaluations for the user. We can do the evaluation on test data without the fear of leakage because we do not use the results to make changes to the model. Every user $U_i$ then trains their model on their training data to get the updated weights $W_i$ for their $M_i$. After the training, the evaluation is conducted once again on the test data and then stored in the collection of $post\_fit$ evaluations. The updated weights $W_i$ are returned by every user are stored in the main algorithm. This set of weights is passed in to the averaging function that returns an average of the weights called $W_{avg}$. The averaging function is based on equation~\ref{eq:avg}. These weights are then passed onto the users in the next round of federated learning, where they initialise their models with the new averaged weights and conduct the whole process again. After a set number of rounds, the federated learning process ends. In this project, we decided to use a high number so we could observe how the process runs over a long period of time. 
\subsubsection{Implementation}
The first thing that had to be implemented was the User class. It is used to store all the data local to a user. Instances of this user class are used to represent an arbitrary user $U_i$. The data stored on $U_I$ includes the training, validation and test data, the model of the neural network and the history of the metrics obtained during the training of the model. The training, validation and test data are stored as numpy arrays. Numpy arrays were used over standard Python array lists because of their vectorised operations which make them faster and more concise than using Python's array based lists. Another reason to user numpy arrays was because of the fact that Keras models require their input to be numpy arrays or pandas DataFrames. Storing the data as numpy arrays standardises the structure of the object for use throughout the project and increases re-usability of the code. The Keras model is stored in $U_i$ as well along with the $pre\_fit$ and $post\_fit$ metrics which are stored in numpy arrays. The UML diagram for this class can be seen in Figure~\ref{fig:classes_user}. 
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{resources/classes_User.png}
	\caption{UML class diagram for the class User.}
	\label{fig:classes_user}
\end{figure}
\noindent Then for every user in the dataset, we must instantiate a user object $U_i$ to represent them. The data relevant to the user is stored in this object after that. The details of this process will be explored in Section~\ref{sec:experiments}. After initialising the users, implementing the main algorithm was the next goal. 
\\\\
The algorithm starts off by sending $M$ to all the users which would ordinarily be done in a networked fashion but since we are using simulated users, a simple for loop suffices. In the loop, a copy of the compiled Keras model $M$ is stored in every $U_i$ which the users then refer to as $M_i$. After doing so, the federated learning process can begin. This process was implemented as a function, \texttt{train\_fed}, to increase re-usability throughout the project. This function would take in arguments such as \texttt{epochs}, \texttt{rounds} and more, to allow for flexibility in terms of how to run the training process. In it we conduct a set number of rounds of federated learning in which the training of the user's model and the sharing and averaging of the weights takes place.
\\\\
To train all the user models, we iterate over all the users and call the \texttt{train} method on every $U_i$. In the \texttt{train} method, we make use of the API Keras provides for its models. Using the \texttt{set\_weights} method of $M_i$, we can set the weights of the model to be the weights that we pass into the method. For the first round of federated learning, we do not set the weights to be anything new. But for subsequent rounds, the averaged weights $W_{avg}$ are passed into the \texttt{set\_weights} method which set the weight of $M_i$ to be $W_{avg}$. After doing so, the model is evaluated on the test data by using the \texttt{evaluate} method of $U_i$. This method intern uses the \texttt{evaluate} method provided by Keras to run evaluation on their models on give data, test data in our case. This data is then stored in the numpy arrays dedicated to storing $pre_fit$ metrics such as \textit{accuracy} and \textit{loss}. Then a simple call to the \texttt{fit} method of the model will train it on the training data that we pass into the method for a given number of epochs. After the training, we once again call the \texttt{evaluate} method of $U_i$ to evaluate the model on the test data and record the metrics in the numpy arrays. As mentioned before, Keras also maintains history of the metrics during the training process in the model object. This history is reset every time the \texttt{fit} method is called. So, after every local training process we store the metrics from this history in $U_i$ as well in a Python list. We can use this information to plot more graphs and review the progress of the learning process, but this time on a more granular level because this is a history of \textit{epoch} evaluations. The $pre_fit$ and $post_fit$ evaluations we store are \textit{round based} evaluations.
\\\\
Once all the users have been trained, their weights need to be averaged. To make the averaging process more modular, an class dedicated to averaging was implemented. This class is called Average. The UML diagram for the final version of the class Average can be seen in Figure~\ref{fig:classes_average}.
\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{resources/classes_Average.png}
	\caption{UML class diagram for the class Average.}
	\label{fig:classes_user}
\end{figure}
\noindent Residing in this class are static methods that deal with different kinds of averaging, one of which is the implementation of the averaging function in equation~\ref{eq:avg}. The method is provided with a reference to all $U_i$ in the form of a dictionary. With access to $U_i$ comes access to model $M_i$ for the user as well. The \texttt{get\_weights} method associated to $M_i$ is used to extract the weights $W_i$ from model $M_i$. This is done for every user and then those weights are used to calculate the averaged weights $W_{avg}$. Keras models store their weights as a list of numpy arrays. To be able to use numpy's quick calculations we cast the list into a numpy array of numpy arrays. To calculate the average, we iterate over all the users. When doing so we use the \texttt{get\_weights} method to get the weights (which is a list of numpy arrays), cast it to a numpy array of numpy arrays and then take the sum of the weights from all the users. This means that at the end of the loop we can divide by the number of users and get $W_{avg}$. $W_{avg}$ is then returned into the main function \texttt{train\_fed} where it will be fed back to all the users for the next round of federated learning where they initialise their models $M_i$ with weights $W_{avg}$. After a set number of federated learning rounds have been performed, we can stop the process and plot graphs from the metrics we had stored along the way. Our choice of choosing a high number of rounds lets us know at exactly what point the metrics start plateauing, and we can then say that training after that point is not necessary. 
\subsection{TensorFlow Federated}\label{subsec:tff}
In this section we will talk about simulating federated learning using TensorFlow Federated (TFF). The design aspect of this will not be discussed as it is the same as the federated approach we just discussed. Instead, we will focus on the implementation of federated learning using TFF. We implemented this approach for it to serve as a sanity check to see if our implementation as described in Section~\ref{subsec:fedml-core} is sound. 
\\\\
TensorFlow Federated is a framework for that allows for computation on decentralised data. TFF essentially has its own underlying language that can be accessed with Python. The building blocks of which are federated computations, which are computations that take place in a federated setting. TFF has two layers, the Federated Learning (FL) layer and the Federated Core (FC) layer. The Federated Learning layer allows high level plugging of Keras models into the TFF framework. We can perform basic tasks like the federated training process or evaluation without having to deal with the lower level concepts. The Federated Core layer provides us with low level control over the algorithms used in federated learning. In this section, we will be using the FL layer and not the FC layer as we do not need to implement custom algorithms. 
\\\\
TFF provides detailed documentation and tutorials on how to implemented federated learning. The implementation here is based on those tutorials. We will assume that the dataset here is preprocessed and ready to be used, details on how this was achieved for experiments can be seen in Section~\ref{sec:experiments}. The basic idea is that every user's data is to be put into a Dataset object that is part of TensorFlow's data representation library \texttt{tff.data}. The Dataset object corresponding to every user is then put into a Python list which will be used going forward. A model architecture is then defined using Keras, but unlike the other approaches we have seen so far, we do not compile the model. This is done at a later stage in the process. A function is defined that constructs and returns a Keras model. We call it \texttt{create\_model}. For models to be used in TFF, they need to be wrapped in a Model interface that is part of TFF's learning library \texttt{tff.learning}. This interface exposes methods that TFF needs to carry out training and other federated operations on the model in a federated setting. TFF will wrap the model for us by invoking a call to the function \texttt{tff.learning.from\_keras\_model}. We pass the non-compiled model from \texttt{create\_model} into this function, along with other parameters like the loss function, metrics functions and a sample of the dataset. The sample of the dataset is provided for initialisation purposes. This process of constructing the model and wrapping the model is placed into a function as well. We call this function \texttt{model\_fn}.
\\\\
We can now initiate the federated learning process with TFF. A simple call to the function \texttt{tff.learning.build\_federated\_averaging\_process} with \texttt{model\_fn} as the parameter constructs a Federated Averaging algorithm and packages everything into a \texttt{tff.utils.IterativeProcess} which we call \texttt{iterative\_process}. This constructs everything that needs to be done on the central agent and on the user's end, including compiling the model distributing them to users. It essentially implements everything that takes place in a round loop iteration in algorithm~\ref{algo:fedcore}. We can now initialise \texttt{iterative\_process} by calling the \texttt{initialize} method on it which returns the server state which we call \texttt{state}. A single round of federated averaging is performed by making a call to the \texttt{next} method on the initialised \texttt{iterative\_process} object. We pass \texttt{state} and the training data into the \texttt{next} method which returns the updated server state \texttt{state} and the metrics at the end of the round. The method call causes all the local training on the simulated users, sharing and averaging of the weights and reinitialising the users models with the averaged weights to take place. Running the \texttt{next} method for a number of times representing the rounds of federated learning we want to perform. 
\\\\
At the end of the training, we must evaluate the model. For that we make a call to \texttt{tff.learning.build\_federated\_evaluation} and pass in the \texttt{model\_fn} we defined. This initialises the federated computations that need to take place for this process returns the corresponding federated computation object. We pass in the trained model and testing data into this to receive the metrics of the evaluation of the model on the training data. The trained model containing the averaged weights can be accessed from the server state that is returned after a round of federated learning. These metrics represent the performance of this implementation of federated learning using TFF. The comparison of the TFF implementation and our implementation of federated learning can be seen in Section~\ref{sec:experiments}. The results showed that the two implementation did not differ massively, which meant that our implementation was logically sound and that we could use it for further experiments.
\\\\
We decided not to use the TFF implementation because of the learning curve required in working with the FC layer of TFF. It was more intuitive to work with a sound implementation that we had built up from scratch instead of having to hack together our extended ideas using the TFF framework. 
\clearpage
\section{Extended Ideas}\label{sec:ext-ideas}
\subsection{Central Server}
\subsubsection{Weighted Average}
\subsubsection{Excluding based on std dev}
\subsubsection{Local only}
\subsection{Peer to Peer}
\subsubsection{Weighted Average}
\subsubsection{Excluding based on std dev}
\subsubsection{Local only}
\clearpage

\section{Evaluation}\label{sec:experiments}


tensorflow gpu


Experiments using two distinct datasets were conducted to compare the performance of all the ideas. The first dataset was based on hand gestures and the second dataset was based on images. These datasets, the experiments conducted on them and the results obtained are described in Sections~\ref{subsec:gestureset} and~\ref{subsec:imageset} respectively.
\\\\
To make the whole process as fair as possible, a model that performed well on a given dataset with the traditional ML approach was found emperically. Using the traditional approach, a model was found whose performance was good.


So before using the splitting logic on it, we must first read in the dataset using pandas and split it based on the users. The logic behind this is specific to the dataset, and will be discussed in Section~\ref{sec:experiments} where we conduct experiments on actual datasets
• Comoing up with decent model
• Traditional ml results
\subsection{tff}
Since the data is already a tf.data.Dataset, preprocessing can be accomplished using Dataset transformations. Here, we flatten the 28x28 images into 784-element arrays, shuffle the individual examples, organize them into batches, and renames the features from pixels and label to x and y for use with Keras. We also throw in a repeat over the data set to run several epochs.

talk about hdf5
]
\subsection{trainmodel}
. epoch*rounds = rounds

We have to then try to find a model $M$ that performs well on the dataset. Keras provides an API to construct a model with relative ease. It allows us to construct a layer of neurons and stack them on top of each other to build an architecture suitable for our needs. The layers include, but are not limited to, the layers mentioned in Section~\ref{subsec:arch}. To find the best $M$, several different architectures were tested on the dataset in question. The tests included training the model on the training data and then plotting a graph of metric versus epoch to see how the metrics progress over time. The metric of choice was generally accuracy. The plots were made for both, the performance of the model on the training data and the validation data. Efforts were made to ensure that the model was not suffering from over-fitting or under-fitting. The plots were very useful in checking for that. An indication of over-fitting is that the metrics of the validation data and training data begin to diverge. In this case, the model is made a bit simpler. Techniques include reducing the number of neurons, number of layers, adding regularization or adding a dropout layer (where certain neuron outputs are ignored). A sign of under-fitting is that the metrics are too ``bad", for instance the accuracy being too low. In this case, we can do the opposite of what we do in the case of over-fitting. All of these techniques can be easily implemented in Keras using the API it provides to create and edit layers. 

\subsection{mpdel}
Once the data is ready, it is essential to find a model $M$ that performs well on the given data. The process of finding an $M$ that fits the dataset well is an empirical one. It involves numerous experiments with different model architectures and tweaking of hyper-parameters. The model with the best metrics on the validation data is selected as $M$. This is a crucial step, as when we conduct experiments with the same dataset in a federated setting, the same model $M$ will sent to every user $U_i$ during the initialisation process. The reason behind this is to have a fair set of experiments where the model is not specifically chosen to work well in a federated environment. 
\subsubsection{Data separation}\label{subsubsec:datasep}
\textbf{\LARGE INCOMPLETE}
After explaining how to split the data for a given user, we need to look at how to allocate data to a specific user in the first place. Datasets can come in many shapes and forms. Some datasets are structured in csv files and some datasets are structured using a directory structure. Sometimes they come with explicit user identities, specifying what part of the dataset belongs to what user, and some come without any user identity. Here we will explain at two ways in which datasets are delivered that are most relevant to this project.
\\\\
In the case where the dataset is delivered in a csv file with a column dedicated to indicate the user ID that a sample belongs to, the separation is trivial. The idea is to iterate over the csv file and based on the user IDs, separate the data. To do so, the csv file is read into a pandas DataFrame. We can call this $df$. As the user ID column indicates what user a sample of data belongs to, filtering $df$ on the user ID column is enough to separate the data. Pandas DataFrames have a filter functionality which allows us to do this with ease. First, we find the unique user IDs in the column which we can easily do using the DataFrame API. We then iterate over all the user IDs and filter $df$ so it return DataFrames that contain data only for a particular user. We maintain an array of references to these DataFrames and return it for further processing to split it as in Section~\ref{subsubsec:split}. Algorithm~\ref{algo:datasep-csv} shows the same in a Python based pseudocode below.
\begin{center}\begin{algorithm}[H]
\Fn{\FDataSepCsv{df}}{
	user\_dfs = [ ]\\
	\# returns a list of unique user IDs\\
	user\_ids = df["UserID"].unique() \\
	\For{\forcond{$user_id$}{$user_ids$}}{
		\# returns a DataFrame with data for user with ID $user_id$\\
	    df\_user = df[df["UserID"] == user\_id]\\
		user\_dfs.append(df\_user)
    }
  	\textbf{return} user\_dfs
  \caption{Data separation for csv files with user IDs}\label{algo:dataset-csv}
}
\end{algorithm}\end{center}
In the case where the dataset is delivered using a directory structure and no user IDs, we have some freedom in terms of the separated. We can use the idea of \textit{simulated federated data}. This is where artificial users are created and given certain characteristics. A sample of a characteristic which we will look at later in section~\ref{subsec:imageset} is image affinity. We can create users have a higher affinity to a certain class of images more than classes, i.e., a user who has 80\% of their images as cats and the other 20\% are random pictures from the given dataset. The 80-20 ratio, as mentioned previously, can be changed to any other ratio we may want to experiment with because it is was artificially set by us in the first place. The directory structure indicates the classes of images, for instance, images in a directory called \textit{cat} are labelled as \textit{cat}.
\\\\
To implement the simulated federated data idea, we 

\begin{center}\begin{algorithm}[H]
\Fn{\FDataSepDir{dir}}{
	\# valid extensions\\
    VALID\_IMAGE\_EXTENSIONS = [".jpg", ".jpeg", ".png"]\\
	user\_ids = df["UserID"].unique()\\
	\For{\forcond{$user\_id$}{$user\_ids$}}{
		\# returns a DataFrame with data for user with ID $user\_id$\\
	    df\_user = df[df["UserID"] == user\_id]\\
		user\_dfs.append(df\_user)
    }
  	\textbf{return} user\_dfs
  \caption{Data separation for csv files with user IDs}\label{algo:dataset-simulated}
}
\end{algorithm}\end{center}


The implementation begins the same way by splitting the data as in Section~\ref{subsubsec:split} and storing it in user objects defined in Python as seen in Section~\ref{subsubsec:datarep}. But first, we must deal with the fact that the dataset provided here is a combination of every user's data in one csv file. So before using the splitting logic on it, we must first read in the dataset using pandas and assign users the data that should be local to them. The logic behind this is specific to the dataset being used, and will be discussed in Section~\ref{subsec:datasep} where we conduct experiments on actual datasets. But the crux of the matter is to read in the dataset and separate the data specific to every user and then apply to splits to the user's local data.


\subsection{data separation}
With that in mind, we talk a little bit about the percentage of user data present in the three categories that we mentioned above. We talk about this here specific to the . There are two ways to split user based data, the na\"ive way and a more sophisticated way based on the idea of stratification. The na\"ive way is to take the whole dataset and just split it into training, validation and test data. If this were to be the case, then it is almost certain that there would be user data which would end up not being present in a given split. For example, it would lead to a situation where data from user $U_i$ would not be present in the training data at all, but possibly take up the entirety of the test data. One could suggest that shuffling the dataset before the splitting would fix this issue, but that is not the case. It is better than the plain na\"ive approach described above, but data from a user $U_i$ might still end up being under represented in either of the three categories of data that we store.
\\\\
The more sophisticated approach which based on the idea of stratification addresses the issue of users being under represented in the training, validation or test data. In this approach, we split every user's data into the three categories. Then, for each categories, we take the union categorical data for every user. This union then acts as the data for the $global\_user$. For example, after splitting every user's data, we will take every user's test data and union it to get a collection of test data from all the users. This collection of test data acts as the test data for the the $global\_user$. With this approach we can ensure that every user is represented in accordance with the size of their dataset. This can be visualised in Figure~\ref{fig:split}.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{resources/split.png}
	\caption{A visual representation of the data splitting approaches.}
	\label{fig:split}
\end{figure}
% clearly bad
\noindent To store this data, we use the idea of a User object which is described in Section~\ref{subsubsec:datarep}. Algorithm~\ref{algo:strat_collection} illustrates the stratified split idea well by using pseudocode. For every user, the data is split and added to a collection of the same type of data. After iterating through all the users, a $global\_user$ object as described in Section~\ref{subsubsec:datarep} is instantiated and the data stored in it. Then the $global_user$ object is returned for further usage. 
\begin{center}\begin{algorithm}[H]
\Fn{\FDataSplit{users}}{
	unioned\_train = [ ]\\
	unioned\_validation = [ ]\\
	unioned\_test = [ ]\\
    \For{\forcond{$U_i$}{$users$}}{
	    train, validation, test = split\_user\_data($U_i$)\\
	    unioned\_train.append(train)\\
	    unioned\_validation.append(validation)\\
	    unioned\_test.append(test)\\
    }
    global\_user = User()\\
    global\_user.set\_train\_data(unioned\_train)\\
    global\_user.set\_validation\_data(unioned\_validation)\\
    global\_user.set\_test\_data(unioned\_test)\\
    \textbf{return} global\_user
  \caption{Stratified data collection for the global user}\label{algo:strat_collection}
}
\end{algorithm}\end{center}

\subsection{Training, validation and testing splits}\label{subsubsec:split}
As a rule of thumb, any dataset provided needs to be split up into the three parts, training, validation and testing data to avoid leakage. A common strategy was used through the project to ensure consistency amongst all experiments. This strategy is explained in this section.
\\\\
The dataset provided is usually in a csv file and is read into a DataFrame using the API that pandas provides. To have flexibility, we extract the data as a numpy array from the DataFrame and work directly with the array. We split the array into three parts, training, validation and test data, where they have 60\%, 20\% and 20\% of the data respectively. This split ratio is the industry standard when it comes to three splits like we do in this project. Every split has a specific task. The training data is used exclusively for training the model. The validation data is used to see how the model performs on unseen data that is not the test data. We can inspect this data, analyse how the model performs on it and tweak the model appropriately. We do not do the same with the test data. The purpose of having a validation set is to avoid leaking knowledge of the test set into the training process of the model. The validation set is not directly used in the training of the model. The test set is used at the very end to evaluate the final performance of the model.
\\\\
To implement the splitting, context specific functions were defined each of which would be used based on the input data type. The parameters for the functions would include \texttt{full\_data}, \texttt{for\_user}, \texttt{val\_size}, \texttt{test\_size} and \texttt{seed}. The values for \texttt{test\_size} and \texttt{val\_size} in this project were 0.2 and 0.2 respectively, giving giving 0.6 to the training set. These fractions represent the fraction of the whole input dataset to be allocated to the three named subsets. The logic of the splitting requires the input data to first be split into training and test data on the specified split ratios of 0.8 and 0.2 respectively. Then the training data must be split into training and validation data. As the split now takes place on 80\% of the input data, we need to adjust \texttt{val\_size} to ensure that the correct split ratios are maintained with respect to the full input data size. To do so, \texttt{val\_size} needs to be recalculated as in equation~\ref{eq:val_size}. The training set is then split into training and validation data based on the new \texttt{val\_size}.
%	\texttt{val\_size} = \frac{\texttt{val\_size}}{\texttt{1-test\_size}}
\begin{equation}\label{eq:val_size}
	val\_size = \frac{val\_size}{1-test\_size}
\end{equation}
The \texttt{seed} ensures reproducibility because of the shuffling involved in the splitting method provided by scikit-learn. The parameter \texttt{for\_user} is used to split the data pertaining to only a specific user in the dataset.


\subsection{Designing the framework}
The testing framework was fairly simple, it involved iterating over the different scenarios and recording the results. There were 
\subsection{Gestures dataset}\label{subsec:gestureset}
\subsubsection{Description}
found issues so moved on
\subsection{Image dataset}\label{subsec:imageset}
\subsubsection{Description}
h5 files
scaling
artificial users
\clearpage
\subsection{Graphing}
For each dataset show the otuput we got
talk about tensorflwo gpu
\subsection{Testing user weights on global data}
splitting of data 

Analysis\\
Design\\
Implementation\\
Evaluation\\
Conclusions\\

third person
\\
present or past tense

design talks about apckages and stuff. convince them its good
describe test set
Can just saw 6 other graphs were xyz for whatever reason
Give a reason to why im shoeing something

\clearpage
\section{Conclusions and future work}
can use "i" in here

early stopping
could have implemented the exact version of google with deltas being sent
secure aggregation could have been explored


\section{notes}
Not written in the passive\\
	Name the agent\\
	Describe the system\\
		Give it a name\\\\
		User\\\\
		Name parts of the system\\ 
		Edge agent, central agent\\
Present and past\\
Implemenation\\
Heres the code using keras
\\Conv and non conv
\\Say how keras gets weight by making refernce to algorithms
\\\\
\clearpage
\printbibliography[title={Bibliography}]


\end{document}

